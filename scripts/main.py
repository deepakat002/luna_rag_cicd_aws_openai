import chainlit as cl
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

load_dotenv()  # Load your OpenAI key etc.

# -------------------- Load Vector Store --------------------
def load_vectorstore():
    try:
        print("ğŸ”„ Loading PDF...")
        loader = PyPDFLoader("../data/beagle.pdf")
        docs = loader.load()

        print(f"ğŸ“„ Loaded {len(docs)} pages")

        print("âœ‚ï¸ Splitting chunks...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(docs)

        print(f"ğŸ”¢ Total Chunks: {len(chunks)}")

        print("ğŸ§  Loading Embeddings...")
        embeddings = OpenAIEmbeddings()

        print("ğŸ’¾ Building Chroma Vector DB...")
        vectordb = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma_db")
        vectordb.persist()

        print("âœ… Vectorstore ready.")
        return vectordb

    except Exception as e:
        print("âŒ Error loading vectorstore:", e)
        return None

# -------------------- Start Chat --------------------
@cl.on_chat_start
async def on_chat_start():
    print("ğŸŸ¢ Chat started.")
    vectorstore = load_vectorstore()

    if vectorstore is None:
        await cl.Message("âš ï¸ Could not load knowledge base. Only general chat is available.").send()
        chain = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        cl.user_session.set("chat_chain", chain)
        return

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model="gpt-3.5-turbo", temperature=0),
        retriever=retriever,
        memory=memory,
        verbose=True
    )

    cl.user_session.set("chat_chain", chain)
    await cl.Message("ğŸ‘‹ Hi! Ask me anything about Beagles ğŸ¶").send()

# -------------------- Handle Messages --------------------
@cl.on_message
async def handle_message(message: cl.Message):
    chain = cl.user_session.get("chat_chain")

    if isinstance(chain, ChatOpenAI):
        # fallback mode (no vector)
        print("ğŸŸ¡ Responding without retrieval")
        response = chain.invoke(message.content)
    else:
        # retrieval mode
        print("ğŸ” Responding with vectorstore")
        response = chain.run(message.content)

    await cl.Message(content=response).send()
